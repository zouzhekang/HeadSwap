<h1 align='center'>HeadSwap</h1>

**place your head automaticly.**  
âš ï¸  infact, I made a big mistake at my code, I'll resolve it and retrain an new weight, please wait some days...

## ğŸ“¸ Showcase



## ğŸ“…ï¸ TODO

| Status | Milestone |    ETA     |
| :----: | :---------------------------------------------------------------------------------------------------- | :--------: |
| ğŸš€ | **[Inference source code meet everyone on GitHub](https://github.com/zouzhekang/HeadSwap)** | comming soon |
| ğŸš€ | **[Pretrained model on Huggingface](https://huggingface.co/)**              | comming soon |
| ğŸš€ | **[Releasing data preparation and training scripts](#training)**                                                | TBD |
| ğŸš€ | **[ComfyUI support]()**                                                    | TBD |

## ğŸ”§ï¸ Framework

![framework](assets/framework.jpg)

## âš™ï¸ Installation

- System requirement: centos, Cuda 12.4
- Tested GPUs: A800

Create conda environment:

```bash
  conda create -n headswap python=3.10
  conda activate headswap
```

Install packages with `pip`

```bash
  pip install -r requirements.txt
```

## ğŸ—ï¸ï¸ Usage

The entry point for inference is `inference.py`. Before testing your cases, two preparations need to be completed:

1. [Download all required pretrained models](#download-pretrained-models).
2. [Prepare source image and driving audio pairs](#prepare-inference-data).
3. [Run inference](#run-inference).

### ğŸ“¥ Download Pretrained Models

you can download [dwpose](https://huggingface.co/fudan-generative-ai/hallo/tree/main/hallo) and put it in ./pretrained_models/


Then you can easily get all pretrained models required by inference.py from HuggingFace repo.

Finally, these pretrained models should be organized as follows:

```text
./pretrained_models/
`-- DWPose/
    |-- dw-ll_ucoco_384.onnx
    `-- yolox_l.onnx
./insightface_model/
`-- models/
    `-- buffalo_l/
        |-- det_10g.onnx
        `-- w600k_r50.onnx
~/.cache/
`-- huggingface/
    `-- hub/
        |-- models--Qwen--Qwen-Image-Edit-2509
        `-- 
```

### ğŸ› ï¸ Prepare Inference Data

We have provided [some samples](./assets/) for your reference.

### ğŸ® Run Inference

Simply to run the `./inference.py` and pass `source_image` and `target_image` as input, You can pass `--output` to specify the output file name. 

```bash
python inference.py --source_image assets/source0.jpg --target_image assets/target0.jpg --output ./test.png
```  

## âš ï¸ Social Risks and Mitigations

This project is intended solely for learning, communication, technical research, and legitimate creative expression. I'm fully aware that this technology carries serious social risks if misused to create false information or non-consensual synthetic content. Therefore, I strictly prohibit any form of illegal or malicious use, including but not limited to defaming others, committing fraud, or infringing on personal image rights. Users must comply with laws and regulations, ensure that they have obtained explicit authorization from relevant individuals, and clearly label any generated content. The developers of this project are not responsible for any consequences arising from the misuse of the code by users.  

æœ¬é¡¹ç›®ä»…ç”¨äºå­¦ä¹ äº¤æµã€æŠ€æœ¯ç ”ç©¶ä¸åˆæ³•çš„åˆ›æ„è¡¨è¾¾ã€‚æˆ‘æ·±çŸ¥è¯¥æŠ€æœ¯å­˜åœ¨è¢«æ»¥ç”¨äºåˆ¶ä½œè™šå‡ä¿¡æ¯ã€éåŒæ„æ€§åˆæˆå†…å®¹ç­‰ä¸¥é‡ç¤¾ä¼šé£é™©ã€‚å› æ­¤ï¼Œæˆ‘ä¸¥æ ¼ç¦æ­¢ä»»ä½•å½¢å¼çš„éæ³•æˆ–æ¶æ„ç”¨é€”ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºè¯‹æ¯ä»–äººã€å®æ–½æ¬ºè¯ˆæˆ–ä¾µçŠ¯è‚–åƒæƒã€‚è¯·ä½¿ç”¨è€…åŠ¡å¿…éµå®ˆæ³•å¾‹æ³•è§„ï¼Œç¡®ä¿å·²è·å¾—ç›¸å…³äººç‰©çš„æ˜ç¡®æˆæƒï¼Œå¹¶å¯¹ç”Ÿæˆå†…å®¹è¿›è¡Œæ˜¾è‘—æ ‡è¯†ã€‚æœ¬é¡¹ç›®çš„å¼€å‘è€…ä¸æ‰¿æ‹…ç”±ä½¿ç”¨è€…æ»¥ç”¨ä»£ç è€Œå¼•èµ·çš„ä»»ä½•è´£ä»»ã€‚  

## ğŸ¤— Acknowledgements

I would like to thank the contributors to the [Qwen-Image](https://github.com/QwenLM/Qwen-Image), [DiffSynth-Studio](https://github.com/modelscope/DiffSynth-Studio), [insightface](https://github.com/deepinsight/insightface) and [dwpose](https://github.com/IDEA-Research/DWPose) repositories, for their open research and exploration.

If I missed any open-source projects or related articles, I would like to complement the acknowledgement of this specific work immediately.
